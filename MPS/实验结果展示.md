# MPS实验结论和实验结果展示
1. 背景  
    过去Kubernetes服务提供的Nvidia GPU容器调度能力通常都是将一个GPU卡分配给一个容器，于模型开发和模型预测的场景GPU资源就会比较浪费。为了提高集群GPU的利用率，阿里云开源的GPUShare方案利用Kubernetes的Scheduler Extender和Device Plugin机制完成了GPU资源显存级别的划分，在调度层次实现了GPU的共享。Nvidia的MPS服务能够实现GPU实际分配过程中的资源隔离和控制。  
2. 目的  
    测试MPS在GPU资源分配时的隔离和资源控制能力。  
3. 实验过程  
    3.1 tensorflow任务,不在程序里用tf_config.gpu_options.per_process_gpu_memory_fraction进行资源限制。  
    ![Result](https://ws4.sinaimg.cn/large/006tNc79ly1g4s7ubcv4oj315a09mjsm.jpg)  
    3.2 tensorflow任务,在程序里使用动态增长模式，设置tf_config.gpu_options.allow_growth = True。
    ![Result](https://ws4.sinaimg.cn/large/006tNc79ly1g4s7qow9zzj315c09o75e.jpg)  
    3.3 nbody任务。  
    ![Result](https://ws1.sinaimg.cn/large/006tNc79ly1g3xgcy9m3uj318e0h2dhc.jpg)
4. 实验结论  
    利用MPS的CUDA_MPS_ACTIVE_THREAD_PERCENTAGE能力可以实现对nbody任务的资源管控，但是对于tensorflow框架的任务，MPS无法控制其显存使用量，需要用户在程序中手动干预来实现显存管控。
